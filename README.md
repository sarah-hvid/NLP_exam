# NLP Exam - Cognitive Science Msc
This repository contains all code and data related to the exam paper: __Evaluating a Large Language Model in a News Generating Context - Bias and Toxicity__.\
__Warning:__ Examples include harmful and toxic content.

## Assignment description
The purpose of the paper is to investigate bias towards binary gender and ethnicities present in ```Stable Beluga``` as a fine-tuned version of Llama2. Evaluation metrics will be used to compare the model's output in three different conditions; original Llama2 system prompt, no system prompt and an inverted version of the Llama2 system prompt. The metrics employed will cover sentiment scores, toxicity scores and unigram and co-occurrence scores.

## Usage
In order to run the notebooks, certain modules need to be installed. These can be found in the ```requirements.txt``` file. The folder structure must be the same as in this GitHub repository (ideally, clone the repository).\
The data used in the assignment was generated in Google Colab and was generated by the ```article_generation``` notebook.\
All notebooks are located in the ```nbs``` folder. The notebook used to conduct the grid search is calld ```grid_search```. The notebook used to perform matrics analyses is called ```metrics_analysis```.

## Results
The findings revealed that all system prompts contained bias towards binary gender and ethnicity, though less biased in the positive system prompt. The model did exhibit toxicity and it was demonstrated that the implementation of safety procedures to ensure safe and non-toxic content can easily be compromised. Further alignment is needed to ensure that Stable Beluga does not produce unsafe, biased or toxic news content.

## Authors
- Sarah Hvid Andersen (201910230)
- Martine Lind Jensen (201906161)
 
